{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples Using trust-constr\n",
    "\n",
    "Since the trust-constr algorithm was extracted from the `scipy.optimize` library, it uses the [same interface](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) as `scipy.optimize.minimize`. The main different is that everything is imported from `trust_constr` rather than from `scipy.optimize`. The other difference is that the only optimization method available is 'trust-const'. The examples below show how to use trust-constr with a variety of different types of constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trust_constr import minimize, NonlinearConstraint, LinearConstraint, Bounds, check_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Nonlinear Inequality Constraint with Variable Bounds\n",
    "Example 15.1 from [1]\n",
    "\n",
    "Solve:\n",
    "$$\\min_{x,y} f(x,y)=\\frac{1}{2}(x-2)^2+\\frac{1}{2}\\left(y-\\frac{1}{2}\\right)^2$$\n",
    "Subject to:\n",
    "$$(x+1)^{-1}-y-\\frac{1}{4}\\ge0$$\n",
    "$$x\\ge0$$\n",
    "$$y\\ge0$$\n",
    "\n",
    "Solution: $$(x,y) = (1.953, 0.089)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First solve without defining gradient (finite difference gradient will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution = [1.95282327 0.08865882]\n",
      "Obtained using 42 objective function evaluations.\n"
     ]
    }
   ],
   "source": [
    "def objective(x):\n",
    "    return 0.5*(x[0]-2)**2+0.5*(x[1]-0.5)**2\n",
    "\n",
    "def ineq_constraint(x):\n",
    "    return 1/(x[0]+1)-x[1]-0.25\n",
    "\n",
    "# Use np.inf of -np.inf to define a single sided constraint\n",
    "# If there are more than one constraint, that constraints will \n",
    "# be a list containing all of the constraints\n",
    "constraints = NonlinearConstraint(ineq_constraint, 0, np.inf)\n",
    "\n",
    "# set bounds on the variables\n",
    "# only a lower bound is needed so the upper bound for both variables is set to np.inf\n",
    "bounds = Bounds([0,0], [np.inf, np.inf])\n",
    "\n",
    "# define starting point for optimization\n",
    "x0 = np.array([5.0, 1.0])\n",
    "\n",
    "res = minimize(objective, x0, bounds=bounds, constraints=constraints)\n",
    "\n",
    "print(\"Solution =\", res.x)\n",
    "print(f\"Obtained using {res.nfev} objective function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the gradient for objective and constraint and check gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective difference:  4.6325973080698274e-08\n",
      "constraint difference: 4.16430790545208e-09\n",
      "objective difference:  1.4437517266934117e-07\n",
      "constraint difference: 6.893529434781609e-09\n",
      "objective difference:  4.075336515790102e-07\n",
      "constraint difference: 2.2317148956965682e-08\n",
      "objective difference:  1.8769829188344892e-07\n",
      "constraint difference: 1.6041445392508957e-08\n",
      "objective difference:  1.4472822264292806e-07\n",
      "constraint difference: 1.570823582319747e-08\n"
     ]
    }
   ],
   "source": [
    "def objective_gradient(x):\n",
    "    return np.array([(x[0]-2), (x[1]-0.5)])\n",
    "\n",
    "def ineq_gradient(x):\n",
    "    return np.array([-1/((x[0]+1)**2), -1])\n",
    "\n",
    "# check analytical gradients against finite difference gradient\n",
    "# an incorrect analytical gradient is a common cause for lack of convergence to a true minimum\n",
    "for x in np.random.uniform(low=[0,0], high=[10,10], size=(5,2)):\n",
    "    print(\"objective difference: \", check_grad(objective, objective_gradient, x))\n",
    "    print(\"constraint difference:\", check_grad(ineq_constraint, ineq_gradient, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, minimize using the gradient functions that were just test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution = [1.95282328 0.08865881]\n",
      "Obtained using 14 objective function evaluations.\n"
     ]
    }
   ],
   "source": [
    "constraints = NonlinearConstraint(ineq_constraint, 0, np.inf, jac=ineq_gradient)\n",
    "\n",
    "res = minimize(objective, x0, jac=objective_gradient, bounds=bounds, constraints=constraints)\n",
    "\n",
    "print(\"Solution =\", res.x)\n",
    "print(f\"Obtained using {res.nfev} objective function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Nonlinear Equality Constraint\n",
    "Example 15.2 from [1]\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\\min_{x,y} x^2+y^2$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$(x-1)^3 - y^2 = 0$$\n",
    "\n",
    "Solution:\n",
    "$$(x,y)=(1,0)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution = [9.99966899e-01 3.36074169e-09]\n",
      "Obtained using 181 objective function evaluations.\n"
     ]
    }
   ],
   "source": [
    "objective2 = lambda x: x[0]**2 + x[1]**2\n",
    "objective2_gradient = lambda x: np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "eq_constraint = lambda x: (x[0]-1)**3 - x[1]**2\n",
    "eq_gradient = lambda x: np.array([3*(x[0]-1)**2, -2*x[1]]) \n",
    "\n",
    "# Make the upper and lower bound both zero to define an equality constraint\n",
    "constraints = NonlinearConstraint(eq_constraint, 0, 0, jac=eq_gradient) \n",
    "\n",
    "x0 = np.array([5, 2])\n",
    "\n",
    "res = minimize(objective2, x0, jac=objective2_gradient, constraints=constraints)\n",
    "\n",
    "print(\"Solution =\", res.x)\n",
    "print(f\"Obtained using {res.nfev} objective function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Linear Constraint\n",
    "Example problem from [2]\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\\min_{x,y} 100\\left(y-x^2\\right)^2+\\left(1-x\\right)^2$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$x + 2y \\le 1$$\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "$$(x,y)=(0.5022, 0.2489)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution = [0.50220246 0.24889838]\n",
      "Obtained using 45 objective function evaluations.\n"
     ]
    }
   ],
   "source": [
    "objective3 = lambda x: 100.0*(x[1] - x[0]**2)**2.0 + (1 - x[0])**2\n",
    "\n",
    "objective3_gradient = lambda x: np.array([-400*(x[1]-x[0]**2)*x[0]-2*(1-x[0]),\n",
    "                                          200*(x[1]-x[0]**2)])\n",
    "\n",
    "# define the linear constraint\n",
    "A = np.array([[1,2]])\n",
    "constraints = LinearConstraint(A, [-np.inf], [1])\n",
    "\n",
    "x0 = np.array([-1, 2])\n",
    "\n",
    "res = minimize(objective3, x0, jac=objective3_gradient, constraints=constraints)\n",
    "\n",
    "print(\"Solution =\", res.x)\n",
    "print(f\"Obtained using {res.nfev} objective function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Unconstrained Optimization\n",
    "Example problem from [3]\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\\min_{\\mathbf{x}} \\sum^{N/2}_{i=1}\\left[100\\left(x^2_{2i-1}-x_{2i}\\right)^2+\\left(x_{2i-1}-1\\right)^2\\right]$$\n",
    "\n",
    "Solution for $N=3$:\n",
    "\n",
    "$$\\mathbf{x} = (1,1,1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution = [0.99999729 0.99999458 0.99998915]\n",
      "Obtained using 224 objective function evaluations.\n"
     ]
    }
   ],
   "source": [
    "def rosenbrock_function(x):\n",
    "    result = 0\n",
    "\n",
    "    for i in range(len(x) - 1):\n",
    "        result += 100 * (x[i + 1] - x[i] ** 2) ** 2 + (1 - x[i]) ** 2\n",
    "\n",
    "    return result\n",
    "\n",
    "x0 = np.array([0.1, -0.5, -5.0])\n",
    "\n",
    "res = minimize(rosenbrock_function, x0)\n",
    "\n",
    "print(\"Solution =\", res.x)\n",
    "print(f\"Obtained using {res.nfev} objective function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Nocedal, Jorge, and Stephen J. Wright. *Numerical Optimization*. 2nd ed. Springer Series in Operations Research. New York: Springer, 2006.\n",
    "\n",
    "[2] https://www.mathworks.com/help/optim/ug/fmincon.html\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Rosenbrock_function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
